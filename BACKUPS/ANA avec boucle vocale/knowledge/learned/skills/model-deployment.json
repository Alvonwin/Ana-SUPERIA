{
  "category": "model-deployment",
  "version": "1.0.0",
  "generatedBy": "Claude (Opus 4.5)",
  "description": "ML Model Deployment Patterns",
  "skills": [
    {"id": "mdp01", "type": "basics", "name": "Model Deployment Basics", "description": "Serve ML models", "pattern": "Train → Package → Deploy", "example": "Production ML"},
    {"id": "mdp02", "type": "packaging", "name": "Model Serialization", "description": "Save trained models", "pattern": "pickle, joblib, ONNX", "example": "Model persistence"},
    {"id": "mdp03", "type": "packaging", "name": "Model Containerization", "description": "Docker for ML", "pattern": "Dockerfile + model", "example": "Portable deployment"},
    {"id": "mdp04", "type": "packaging", "name": "ONNX Format", "description": "Interoperable format", "pattern": "Framework agnostic", "example": "Cross-platform"},
    {"id": "mdp05", "type": "serving", "name": "REST API Serving", "description": "HTTP endpoints", "pattern": "FastAPI, Flask", "example": "Synchronous inference"},
    {"id": "mdp06", "type": "serving", "name": "gRPC Serving", "description": "High performance RPC", "pattern": "Protocol Buffers", "example": "Low latency"},
    {"id": "mdp07", "type": "serving", "name": "Batch Inference", "description": "Offline predictions", "pattern": "Scheduled jobs", "example": "Bulk scoring"},
    {"id": "mdp08", "type": "serving", "name": "Streaming Inference", "description": "Real-time events", "pattern": "Kafka + model", "example": "Event-driven ML"},
    {"id": "mdp09", "type": "platforms", "name": "TensorFlow Serving", "description": "TF model server", "pattern": "SavedModel format", "example": "Production TF"},
    {"id": "mdp10", "type": "platforms", "name": "TorchServe", "description": "PyTorch server", "pattern": "Model archive", "example": "Production PyTorch"},
    {"id": "mdp11", "type": "platforms", "name": "Triton Inference", "description": "NVIDIA server", "pattern": "Multi-framework", "example": "GPU inference"},
    {"id": "mdp12", "type": "platforms", "name": "BentoML", "description": "ML service framework", "pattern": "Bentos + serving", "example": "Unified deployment"},
    {"id": "mdp13", "type": "platforms", "name": "Seldon Core", "description": "K8s ML deployment", "pattern": "Inference graphs", "example": "Cloud-native ML"},
    {"id": "mdp14", "type": "platforms", "name": "KServe", "description": "Serverless inference", "pattern": "K8s + Knative", "example": "Autoscaling ML"},
    {"id": "mdp15", "type": "cloud", "name": "SageMaker Endpoints", "description": "AWS inference", "pattern": "Managed serving", "example": "Real-time + batch"},
    {"id": "mdp16", "type": "cloud", "name": "Vertex AI Endpoints", "description": "GCP inference", "pattern": "AutoML + custom", "example": "Google Cloud ML"},
    {"id": "mdp17", "type": "cloud", "name": "Azure ML Endpoints", "description": "Azure inference", "pattern": "Managed online", "example": "Enterprise ML"},
    {"id": "mdp18", "type": "edge", "name": "TensorFlow Lite", "description": "Mobile ML", "pattern": "Quantized models", "example": "Android/iOS"},
    {"id": "mdp19", "type": "edge", "name": "Core ML", "description": "Apple ML", "pattern": "iOS native", "example": "iPhone inference"},
    {"id": "mdp20", "type": "edge", "name": "ONNX Runtime", "description": "Cross-platform", "pattern": "Optimized inference", "example": "Edge devices"},
    {"id": "mdp21", "type": "optimization", "name": "Model Quantization", "description": "Reduce precision", "pattern": "FP32 → INT8", "example": "Smaller, faster"},
    {"id": "mdp22", "type": "optimization", "name": "Model Pruning", "description": "Remove weights", "pattern": "Sparse models", "example": "Compression"},
    {"id": "mdp23", "type": "optimization", "name": "Knowledge Distillation", "description": "Smaller student model", "pattern": "Teacher → Student", "example": "Model compression"},
    {"id": "mdp24", "type": "optimization", "name": "Model Compilation", "description": "Optimized execution", "pattern": "XLA, TVM", "example": "Hardware acceleration"},
    {"id": "mdp25", "type": "patterns", "name": "Canary Deployment", "description": "Gradual rollout", "pattern": "Small traffic %", "example": "Safe updates"},
    {"id": "mdp26", "type": "patterns", "name": "Shadow Deployment", "description": "Parallel inference", "pattern": "Compare models", "example": "Validation"},
    {"id": "mdp27", "type": "patterns", "name": "Multi-Model Serving", "description": "Multiple models", "pattern": "Model selection", "example": "A/B testing"},
    {"id": "mdp28", "type": "patterns", "name": "Model Ensemble", "description": "Combine predictions", "pattern": "Voting, stacking", "example": "Better accuracy"},
    {"id": "mdp29", "type": "monitoring", "name": "Latency Monitoring", "description": "Response time", "pattern": "P50, P99 metrics", "example": "Performance SLA"},
    {"id": "mdp30", "type": "monitoring", "name": "Throughput Monitoring", "description": "Requests per second", "pattern": "Load metrics", "example": "Capacity planning"}
  ]
}
