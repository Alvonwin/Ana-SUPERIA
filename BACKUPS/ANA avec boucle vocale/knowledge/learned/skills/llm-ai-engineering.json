{
  "category": "llm-ai-engineering",
  "version": "1.0.0",
  "generatedBy": "Claude (Opus 4.5)",
  "skills": [
    {"id": "llm01", "type": "basics", "name": "LLM Basics", "description": "Large Language Models", "pattern": "Transformer, pretrained, generation", "example": "AI foundation"},
    {"id": "llm02", "type": "prompting", "name": "Prompt Engineering", "description": "Craft prompts", "pattern": "System, user, assistant roles", "example": "Guide AI"},
    {"id": "llm03", "type": "fewshot", "name": "Few-Shot Learning", "description": "Examples in prompt", "pattern": "Provide examples, in-context", "example": "No training"},
    {"id": "llm04", "type": "cot", "name": "Chain of Thought", "description": "Step-by-step reasoning", "pattern": "Let's think step by step", "example": "Better reasoning"},
    {"id": "llm05", "type": "rag", "name": "RAG", "description": "Retrieval-Augmented Generation", "pattern": "Retrieve context, generate", "example": "Grounded responses"},
    {"id": "llm06", "type": "embedding", "name": "Embeddings", "description": "Vector representations", "pattern": "Semantic similarity, search", "example": "Text vectors"},
    {"id": "llm07", "type": "vectordb", "name": "Vector Databases", "description": "Embedding storage", "pattern": "Pinecone, Chroma, Weaviate", "example": "Similarity search"},
    {"id": "llm08", "type": "chunking", "name": "Text Chunking", "description": "Split documents", "pattern": "Semantic chunks, overlap", "example": "RAG prep"},
    {"id": "llm09", "type": "finetuning", "name": "Fine-tuning", "description": "Customize models", "pattern": "Train on custom data", "example": "Specialized AI"},
    {"id": "llm10", "type": "lora", "name": "LoRA", "description": "Efficient fine-tuning", "pattern": "Low-rank adaptation", "example": "Cheap training"},
    {"id": "llm11", "type": "rlhf", "name": "RLHF", "description": "Human feedback", "pattern": "Reward model, PPO", "example": "Align AI"},
    {"id": "llm12", "type": "agents", "name": "AI Agents", "description": "Autonomous AI", "pattern": "Tools, planning, memory", "example": "Task automation"},
    {"id": "llm13", "type": "function", "name": "Function Calling", "description": "Tool use", "pattern": "API calls, structured output", "example": "AI actions"},
    {"id": "llm14", "type": "langchain", "name": "LangChain", "description": "LLM framework", "pattern": "Chains, agents, retrievers", "example": "AI apps"},
    {"id": "llm15", "type": "llamaindex", "name": "LlamaIndex", "description": "Data framework", "pattern": "Indexes, query engines", "example": "Data + LLM"},
    {"id": "llm16", "type": "streaming", "name": "Streaming", "description": "Real-time output", "pattern": "Token streaming, SSE", "example": "UX improvement"},
    {"id": "llm17", "type": "context", "name": "Context Window", "description": "Token limits", "pattern": "4K, 8K, 128K tokens", "example": "Input size"},
    {"id": "llm18", "type": "memory", "name": "Memory Systems", "description": "Conversation memory", "pattern": "Buffer, summary, vector memory", "example": "Stateful AI"},
    {"id": "llm19", "type": "guardrails", "name": "Guardrails", "description": "Safety controls", "pattern": "Content filters, validation", "example": "Safe AI"},
    {"id": "llm20", "type": "evaluation", "name": "LLM Evaluation", "description": "Measure quality", "pattern": "Benchmarks, human eval", "example": "Quality assurance"},
    {"id": "llm21", "type": "cost", "name": "Cost Optimization", "description": "Reduce expenses", "pattern": "Caching, model selection", "example": "Budget AI"},
    {"id": "llm22", "type": "multimodal", "name": "Multimodal", "description": "Vision + Language", "pattern": "GPT-4V, Claude 3, Gemini", "example": "Images + Text"},
    {"id": "llm23", "type": "local", "name": "Local LLMs", "description": "Run locally", "pattern": "Ollama, llama.cpp, LM Studio", "example": "On-device AI"},
    {"id": "llm24", "type": "quantization", "name": "Quantization", "description": "Model compression", "pattern": "4-bit, 8-bit, GGUF", "example": "Smaller models"},
    {"id": "llm25", "type": "structured", "name": "Structured Output", "description": "JSON output", "pattern": "JSON mode, schemas", "example": "Parseable responses"},
    {"id": "llm26", "type": "routing", "name": "Model Routing", "description": "Choose models", "pattern": "Route by task complexity", "example": "Cost efficiency"},
    {"id": "llm27", "type": "caching", "name": "Semantic Caching", "description": "Cache responses", "pattern": "Similar query detection", "example": "Reduce calls"},
    {"id": "llm28", "type": "observability", "name": "LLM Observability", "description": "Monitor AI", "pattern": "LangSmith, Helicone, traces", "example": "Debug AI"},
    {"id": "llm29", "type": "security", "name": "LLM Security", "description": "Prompt injection", "pattern": "Input validation, sandboxing", "example": "Safe deployment"},
    {"id": "llm30", "type": "deployment", "name": "LLM Deployment", "description": "Production AI", "pattern": "APIs, scaling, monitoring", "example": "Ship AI"}
  ]
}
