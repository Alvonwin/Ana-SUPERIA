{
  "category": "local-llm-deployment",
  "version": "1.0.0",
  "generatedBy": "Claude (Opus 4.5)",
  "description": "Local LLM Deployment - Ollama, LM Studio, vLLM",
  "skills": [
    {"id": "lld01", "type": "ollama", "name": "Ollama Basics", "description": "Run LLMs locally", "pattern": "ollama run model", "example": "ollama run llama3"},
    {"id": "lld02", "type": "ollama", "name": "Ollama Pull", "description": "Download models", "pattern": "ollama pull model:tag", "example": "ollama pull mistral:7b"},
    {"id": "lld03", "type": "ollama", "name": "Ollama API", "description": "REST API usage", "pattern": "localhost:11434/api/generate", "example": "POST with prompt"},
    {"id": "lld04", "type": "ollama", "name": "Ollama Modelfile", "description": "Custom models", "pattern": "FROM, PARAMETER, SYSTEM", "example": "Create custom persona"},
    {"id": "lld05", "type": "lmstudio", "name": "LM Studio", "description": "GUI for local LLMs", "pattern": "Download, configure, chat", "example": "User-friendly"},
    {"id": "lld06", "type": "lmstudio", "name": "LM Studio Server", "description": "OpenAI-compatible API", "pattern": "localhost:1234/v1/chat", "example": "Drop-in replacement"},
    {"id": "lld07", "type": "vllm", "name": "vLLM", "description": "High-throughput serving", "pattern": "PagedAttention, batching", "example": "Production inference"},
    {"id": "lld08", "type": "vllm", "name": "vLLM API Server", "description": "OpenAI-compatible", "pattern": "python -m vllm.entrypoints.api_server", "example": "Fast inference"},
    {"id": "lld09", "type": "llamacpp", "name": "llama.cpp", "description": "C++ inference", "pattern": "CPU/GPU inference", "example": "Efficient GGUF"},
    {"id": "lld10", "type": "gguf", "name": "GGUF Format", "description": "Model format", "pattern": "Quantized models", "example": "Q4_K_M, Q5_K_S"},
    {"id": "lld11", "type": "quantization", "name": "Quantization", "description": "Reduce model size", "pattern": "FP16 → INT8 → INT4", "example": "4-bit models"},
    {"id": "lld12", "type": "context", "name": "Context Length", "description": "Token window", "pattern": "4K, 8K, 32K, 128K", "example": "phi3:mini-128k"},
    {"id": "lld13", "type": "gpu", "name": "GPU Layers", "description": "Offload to GPU", "pattern": "-ngl N for N layers", "example": "Faster inference"},
    {"id": "lld14", "type": "vram", "name": "VRAM Management", "description": "Memory usage", "pattern": "Model size × precision", "example": "7B Q4 ≈ 4GB"},
    {"id": "lld15", "type": "kobold", "name": "KoboldCpp", "description": "llama.cpp GUI", "pattern": "Web interface", "example": "Easy local LLM"},
    {"id": "lld16", "type": "textgen", "name": "Text Generation WebUI", "description": "Oobabooga UI", "pattern": "Gradio interface", "example": "Many backends"},
    {"id": "lld17", "type": "exllama", "name": "ExLlama/ExLlamaV2", "description": "Fast GPU inference", "pattern": "GPTQ/EXL2 models", "example": "Optimized kernels"},
    {"id": "lld18", "type": "gptq", "name": "GPTQ Quantization", "description": "GPU quantization", "pattern": "4-bit GPU models", "example": "Fast inference"},
    {"id": "lld19", "type": "awq", "name": "AWQ Quantization", "description": "Activation-aware", "pattern": "Better quality 4-bit", "example": "Preserves accuracy"},
    {"id": "lld20", "type": "tgi", "name": "Text Generation Inference", "description": "HuggingFace serving", "pattern": "Docker deployment", "example": "Production ready"},
    {"id": "lld21", "type": "openai", "name": "OpenAI-Compatible API", "description": "Standard interface", "pattern": "/v1/chat/completions", "example": "Drop-in replacement"},
    {"id": "lld22", "type": "batching", "name": "Continuous Batching", "description": "Efficient serving", "pattern": "Dynamic batching", "example": "Higher throughput"},
    {"id": "lld23", "type": "speculative", "name": "Speculative Decoding", "description": "Speed up generation", "pattern": "Draft + verify", "example": "2-3x faster"},
    {"id": "lld24", "type": "kvcache", "name": "KV Cache", "description": "Memory optimization", "pattern": "Cache attention states", "example": "Faster generation"},
    {"id": "lld25", "type": "streaming", "name": "Streaming Output", "description": "Token-by-token", "pattern": "Server-sent events", "example": "Real-time display"},
    {"id": "lld26", "type": "multimodal", "name": "Multimodal Local", "description": "Vision + Text", "pattern": "LLaVA, Llama Vision", "example": "Image understanding"},
    {"id": "lld27", "type": "embedding", "name": "Local Embeddings", "description": "Vector embeddings", "pattern": "nomic-embed, bge", "example": "RAG support"},
    {"id": "lld28", "type": "finetune", "name": "Local Fine-tuning", "description": "Train on your data", "pattern": "LoRA, QLoRA", "example": "Custom models"},
    {"id": "lld29", "type": "mlx", "name": "MLX", "description": "Apple Silicon LLMs", "pattern": "Mac M1/M2/M3", "example": "Optimized for Mac"},
    {"id": "lld30", "type": "localai", "name": "LocalAI", "description": "All-in-one local AI", "pattern": "LLM + TTS + STT + Images", "example": "Self-hosted"}
  ]
}
