{
  "category": "message-queues-events",
  "version": "1.0.0",
  "generatedBy": "Claude Opus 4.5 - Module de référence pour Ana SUPERIA",
  "skills": [
    {"id": "mq01", "type": "rabbitmq", "name": "RabbitMQ Connection", "description": "Connect to RabbitMQ broker", "pattern": "const amqp = require('amqplib');\n\nconst connection = await amqp.connect('amqp://user:pass@localhost:5672');\nconst channel = await connection.createChannel();\n\nprocess.on('SIGINT', async () => {\n  await channel.close();\n  await connection.close();\n  process.exit(0);\n});", "example": "Connection with graceful shutdown"},
    {"id": "mq02", "type": "rabbitmq", "name": "Queue Declaration", "description": "Create durable queues", "pattern": "await channel.assertQueue('orders', {\n  durable: true,           // Survive broker restart\n  arguments: {\n    'x-message-ttl': 86400000,  // 24h TTL\n    'x-dead-letter-exchange': 'dlx',\n    'x-max-length': 100000\n  }\n});", "example": "Persistent queue with DLQ"},
    {"id": "mq03", "type": "rabbitmq", "name": "Producer Pattern", "description": "Publish messages to queue", "pattern": "async function publishOrder(order) {\n  const message = Buffer.from(JSON.stringify(order));\n  channel.sendToQueue('orders', message, {\n    persistent: true,\n    contentType: 'application/json',\n    messageId: crypto.randomUUID(),\n    timestamp: Date.now()\n  });\n}", "example": "Persistent messages with metadata"},
    {"id": "mq04", "type": "rabbitmq", "name": "Consumer Pattern", "description": "Consume messages with acknowledgment", "pattern": "channel.consume('orders', async (msg) => {\n  try {\n    const order = JSON.parse(msg.content.toString());\n    await processOrder(order);\n    channel.ack(msg);  // Success\n  } catch (error) {\n    if (msg.fields.redelivered) {\n      channel.nack(msg, false, false);  // Send to DLQ\n    } else {\n      channel.nack(msg, false, true);   // Retry\n    }\n  }\n}, { noAck: false });", "example": "Manual ack with retry logic"},
    {"id": "mq05", "type": "rabbitmq", "name": "Exchange Types", "description": "Route messages with exchanges", "pattern": "// Direct: exact routing key match\nawait channel.assertExchange('orders.direct', 'direct');\nchannel.publish('orders.direct', 'order.created', message);\n\n// Topic: pattern matching\nawait channel.assertExchange('events.topic', 'topic');\nchannel.publish('events.topic', 'order.us.created', message);\nchannel.bindQueue(queue, 'events.topic', 'order.*.created');\n\n// Fanout: broadcast to all\nawait channel.assertExchange('notifications.fanout', 'fanout');", "example": "Direct, topic, fanout exchanges"},
    {"id": "mq06", "type": "kafka", "name": "Kafka Producer", "description": "Publish to Kafka topics", "pattern": "const { Kafka } = require('kafkajs');\n\nconst kafka = new Kafka({\n  clientId: 'order-service',\n  brokers: ['kafka1:9092', 'kafka2:9092']\n});\n\nconst producer = kafka.producer();\nawait producer.connect();\n\nawait producer.send({\n  topic: 'orders',\n  messages: [{\n    key: order.userId,  // Partition key\n    value: JSON.stringify(order),\n    headers: { 'correlation-id': correlationId }\n  }]\n});", "example": "Partitioned, ordered messages"},
    {"id": "mq07", "type": "kafka", "name": "Kafka Consumer Group", "description": "Consume with consumer groups", "pattern": "const consumer = kafka.consumer({ groupId: 'order-processors' });\nawait consumer.connect();\nawait consumer.subscribe({ topic: 'orders', fromBeginning: false });\n\nawait consumer.run({\n  eachMessage: async ({ topic, partition, message }) => {\n    const order = JSON.parse(message.value.toString());\n    await processOrder(order);\n  }\n});", "example": "Parallel processing with groups"},
    {"id": "mq08", "type": "kafka", "name": "Kafka Streams", "description": "Stream processing with Kafka", "pattern": "// Node.js stream processing\nconst { KafkaStreams } = require('kafka-streams');\n\nconst stream = factory.getKStream('orders');\n\nstream\n  .filter(msg => msg.status === 'pending')\n  .map(msg => ({ ...msg, processedAt: Date.now() }))\n  .to('processed-orders');\n\nstream.start();", "example": "Real-time stream transformations"},
    {"id": "mq09", "type": "redis", "name": "Redis Pub/Sub", "description": "Real-time messaging with Redis", "pattern": "const Redis = require('ioredis');\nconst publisher = new Redis();\nconst subscriber = new Redis();\n\n// Publish\nawait publisher.publish('notifications', JSON.stringify({ type: 'alert', message: 'System update' }));\n\n// Subscribe\nsubscriber.subscribe('notifications', (err, count) => {\n  console.log(`Subscribed to ${count} channels`);\n});\n\nsubscriber.on('message', (channel, message) => {\n  const data = JSON.parse(message);\n  handleNotification(data);\n});", "example": "Fire-and-forget pub/sub"},
    {"id": "mq10", "type": "redis", "name": "Redis Streams", "description": "Persistent message streams", "pattern": "// Add to stream\nawait redis.xadd('orders:stream', '*', 'order', JSON.stringify(order));\n\n// Consumer group\nawait redis.xgroup('CREATE', 'orders:stream', 'processors', '0', 'MKSTREAM');\n\n// Read and process\nconst messages = await redis.xreadgroup(\n  'GROUP', 'processors', 'consumer-1',\n  'COUNT', 10, 'BLOCK', 5000,\n  'STREAMS', 'orders:stream', '>'\n);\n\n// Acknowledge\nawait redis.xack('orders:stream', 'processors', messageId);", "example": "Persistent streams with ack"},
    {"id": "mq11", "type": "sqs", "name": "AWS SQS", "description": "AWS Simple Queue Service", "pattern": "const { SQSClient, SendMessageCommand, ReceiveMessageCommand, DeleteMessageCommand } = require('@aws-sdk/client-sqs');\n\nconst sqs = new SQSClient({ region: 'us-east-1' });\n\n// Send\nawait sqs.send(new SendMessageCommand({\n  QueueUrl: QUEUE_URL,\n  MessageBody: JSON.stringify(order),\n  MessageGroupId: order.userId,  // FIFO\n  MessageDeduplicationId: order.id\n}));\n\n// Receive\nconst { Messages } = await sqs.send(new ReceiveMessageCommand({\n  QueueUrl: QUEUE_URL,\n  MaxNumberOfMessages: 10,\n  WaitTimeSeconds: 20\n}));", "example": "AWS managed queue service"},
    {"id": "mq12", "type": "pattern", "name": "Event Sourcing", "description": "Store events as source of truth", "pattern": "class EventStore {\n  async append(streamId, events, expectedVersion) {\n    const stream = await this.getStream(streamId);\n    if (stream.version !== expectedVersion) {\n      throw new ConcurrencyError();\n    }\n    \n    for (const event of events) {\n      await this.db.events.insert({\n        streamId,\n        version: ++stream.version,\n        type: event.type,\n        data: event.data,\n        timestamp: new Date()\n      });\n    }\n    \n    await this.eventBus.publish(events);\n  }\n}", "example": "Immutable event log"},
    {"id": "mq13", "type": "pattern", "name": "Outbox Pattern", "description": "Reliable event publishing", "pattern": "// In same transaction\nawait db.transaction(async (tx) => {\n  await tx.orders.insert(order);\n  await tx.outbox.insert({\n    id: crypto.randomUUID(),\n    aggregateType: 'Order',\n    aggregateId: order.id,\n    eventType: 'OrderCreated',\n    payload: JSON.stringify(order),\n    createdAt: new Date()\n  });\n});\n\n// Separate process publishes from outbox\nconst events = await db.outbox.findUnpublished();\nfor (const event of events) {\n  await eventBus.publish(event);\n  await db.outbox.markPublished(event.id);\n}", "example": "Transactional outbox for consistency"},
    {"id": "mq14", "type": "pattern", "name": "Inbox Pattern", "description": "Idempotent event handling", "pattern": "async function handleEvent(event) {\n  // Check if already processed\n  const existing = await db.inbox.findByEventId(event.id);\n  if (existing) {\n    console.log('Event already processed, skipping');\n    return;\n  }\n  \n  await db.transaction(async (tx) => {\n    // Process event\n    await processOrderCreated(event.data);\n    \n    // Record in inbox\n    await tx.inbox.insert({\n      eventId: event.id,\n      processedAt: new Date()\n    });\n  });\n}", "example": "Prevent duplicate processing"},
    {"id": "mq15", "type": "pattern", "name": "Saga Orchestration", "description": "Coordinate distributed transactions", "pattern": "class OrderSaga {\n  constructor(eventBus, services) {\n    this.eventBus = eventBus;\n    this.services = services;\n  }\n  \n  async execute(order) {\n    const saga = { orderId: order.id, steps: [] };\n    \n    try {\n      // Step 1: Reserve inventory\n      await this.services.inventory.reserve(order.items);\n      saga.steps.push({ step: 'inventory', status: 'completed' });\n      \n      // Step 2: Process payment\n      await this.services.payment.charge(order.total);\n      saga.steps.push({ step: 'payment', status: 'completed' });\n      \n      // Step 3: Ship order\n      await this.services.shipping.create(order);\n      saga.steps.push({ step: 'shipping', status: 'completed' });\n      \n    } catch (error) {\n      await this.compensate(saga);\n      throw error;\n    }\n  }\n  \n  async compensate(saga) {\n    for (const step of saga.steps.reverse()) {\n      await this.services[step.step].rollback();\n    }\n  }\n}", "example": "Orchestrated saga with compensation"},
    {"id": "mq16", "type": "pattern", "name": "Dead Letter Queue", "description": "Handle failed messages", "pattern": "// RabbitMQ DLQ setup\nawait channel.assertExchange('dlx', 'direct');\nawait channel.assertQueue('orders.dlq', { durable: true });\nawait channel.bindQueue('orders.dlq', 'dlx', 'orders');\n\n// Main queue with DLQ\nawait channel.assertQueue('orders', {\n  durable: true,\n  arguments: {\n    'x-dead-letter-exchange': 'dlx',\n    'x-dead-letter-routing-key': 'orders'\n  }\n});\n\n// DLQ processor\nchannel.consume('orders.dlq', async (msg) => {\n  await alertOps('Failed message', msg);\n  await storeForManualReview(msg);\n  channel.ack(msg);\n});", "example": "Capture and handle failures"},
    {"id": "mq17", "type": "pattern", "name": "Priority Queues", "description": "Process messages by priority", "pattern": "// RabbitMQ priority queue\nawait channel.assertQueue('tasks', {\n  durable: true,\n  arguments: { 'x-max-priority': 10 }\n});\n\n// Publish with priority\nchannel.sendToQueue('tasks', message, {\n  priority: task.isUrgent ? 10 : 1,\n  persistent: true\n});", "example": "High priority messages first"},
    {"id": "mq18", "type": "pattern", "name": "Message Deduplication", "description": "Prevent duplicate processing", "pattern": "const processedIds = new Set(); // Or Redis SET\n\nchannel.consume('orders', async (msg) => {\n  const messageId = msg.properties.messageId;\n  \n  // Check dedup cache\n  if (processedIds.has(messageId)) {\n    channel.ack(msg);\n    return;\n  }\n  \n  try {\n    await processOrder(JSON.parse(msg.content));\n    processedIds.add(messageId);\n    // Expire after 24h\n    setTimeout(() => processedIds.delete(messageId), 86400000);\n    channel.ack(msg);\n  } catch (error) {\n    channel.nack(msg, false, true);\n  }\n});", "example": "Idempotent consumers"},
    {"id": "mq19", "type": "pattern", "name": "Request-Reply", "description": "RPC over message queue", "pattern": "// Client\nasync function rpcCall(request) {\n  const correlationId = crypto.randomUUID();\n  const replyQueue = await channel.assertQueue('', { exclusive: true });\n  \n  return new Promise((resolve, reject) => {\n    channel.consume(replyQueue.queue, (msg) => {\n      if (msg.properties.correlationId === correlationId) {\n        resolve(JSON.parse(msg.content));\n      }\n    }, { noAck: true });\n    \n    channel.sendToQueue('rpc.requests', Buffer.from(JSON.stringify(request)), {\n      correlationId,\n      replyTo: replyQueue.queue\n    });\n    \n    setTimeout(() => reject(new Error('RPC timeout')), 30000);\n  });\n}\n\n// Server\nchannel.consume('rpc.requests', async (msg) => {\n  const result = await handleRequest(JSON.parse(msg.content));\n  channel.sendToQueue(msg.properties.replyTo, Buffer.from(JSON.stringify(result)), {\n    correlationId: msg.properties.correlationId\n  });\n  channel.ack(msg);\n});", "example": "Synchronous over async"},
    {"id": "mq20", "type": "pattern", "name": "Fan-Out Pattern", "description": "Broadcast to multiple consumers", "pattern": "// Publisher\nawait channel.assertExchange('events', 'fanout');\nchannel.publish('events', '', Buffer.from(JSON.stringify(event)));\n\n// Each service creates its own queue\nconst queue1 = await channel.assertQueue('email-service-events', { durable: true });\nawait channel.bindQueue(queue1.queue, 'events', '');\n\nconst queue2 = await channel.assertQueue('analytics-service-events', { durable: true });\nawait channel.bindQueue(queue2.queue, 'events', '');", "example": "Multiple subscribers to same event"},
    {"id": "mq21", "type": "nats", "name": "NATS Messaging", "description": "High-performance pub/sub", "pattern": "const { connect, StringCodec } = require('nats');\n\nconst nc = await connect({ servers: 'nats://localhost:4222' });\nconst sc = StringCodec();\n\n// Publish\nnc.publish('orders.created', sc.encode(JSON.stringify(order)));\n\n// Subscribe\nconst sub = nc.subscribe('orders.*');\nfor await (const msg of sub) {\n  const order = JSON.parse(sc.decode(msg.data));\n  console.log(`Received on ${msg.subject}:`, order);\n}", "example": "Lightweight, fast messaging"},
    {"id": "mq22", "type": "nats", "name": "NATS JetStream", "description": "Persistent NATS streams", "pattern": "const js = nc.jetstream();\n\n// Create stream\nawait jsm.streams.add({\n  name: 'ORDERS',\n  subjects: ['orders.*'],\n  retention: RetentionPolicy.Limits,\n  max_msgs: 1000000\n});\n\n// Publish\nawait js.publish('orders.created', sc.encode(JSON.stringify(order)));\n\n// Durable consumer\nconst consumer = await js.consumers.get('ORDERS', 'order-processor');\nconst messages = await consumer.consume();\nfor await (const msg of messages) {\n  await processOrder(msg.json());\n  msg.ack();\n}", "example": "Persistent streams with ack"},
    {"id": "mq23", "type": "pattern", "name": "Circuit Breaker for Queues", "description": "Protect consumers from overload", "pattern": "const circuitBreaker = new CircuitBreaker({\n  failureThreshold: 5,\n  resetTimeout: 30000\n});\n\nchannel.consume('orders', async (msg) => {\n  if (circuitBreaker.isOpen()) {\n    // Put back in queue for later\n    channel.nack(msg, false, true);\n    await sleep(1000);\n    return;\n  }\n  \n  try {\n    await circuitBreaker.execute(() => processOrder(msg));\n    channel.ack(msg);\n  } catch (error) {\n    channel.nack(msg, false, !msg.fields.redelivered);\n  }\n});", "example": "Protect downstream services"},
    {"id": "mq24", "type": "pattern", "name": "Batch Processing", "description": "Process messages in batches", "pattern": "const batch = [];\nconst BATCH_SIZE = 100;\nconst BATCH_TIMEOUT = 5000;\n\nlet flushTimer = null;\n\nfunction scheduleBatchFlush() {\n  if (!flushTimer) {\n    flushTimer = setTimeout(flushBatch, BATCH_TIMEOUT);\n  }\n}\n\nasync function flushBatch() {\n  if (batch.length === 0) return;\n  \n  const items = batch.splice(0, batch.length);\n  await bulkInsert(items.map(m => m.data));\n  items.forEach(m => channel.ack(m.msg));\n  \n  flushTimer = null;\n}\n\nchannel.consume('events', (msg) => {\n  batch.push({ msg, data: JSON.parse(msg.content) });\n  if (batch.length >= BATCH_SIZE) flushBatch();\n  else scheduleBatchFlush();\n});", "example": "Efficient bulk operations"},
    {"id": "mq25", "type": "pattern", "name": "Message Scheduling", "description": "Delayed message delivery", "pattern": "// RabbitMQ delayed message plugin\nawait channel.assertExchange('delayed', 'x-delayed-message', {\n  arguments: { 'x-delayed-type': 'direct' }\n});\n\n// Send with delay\nchannel.publish('delayed', 'orders', message, {\n  headers: { 'x-delay': 60000 } // 60 second delay\n});\n\n// Alternative: TTL + DLQ\nawait channel.assertQueue('scheduled', {\n  arguments: {\n    'x-message-ttl': 60000,\n    'x-dead-letter-exchange': '',\n    'x-dead-letter-routing-key': 'orders'\n  }\n});", "example": "Schedule future processing"},
    {"id": "mq26", "type": "monitoring", "name": "Queue Monitoring", "description": "Monitor queue health", "pattern": "// Get queue info\nconst queueInfo = await channel.checkQueue('orders');\nconsole.log({\n  messages: queueInfo.messageCount,\n  consumers: queueInfo.consumerCount\n});\n\n// Prometheus metrics\nconst queueDepth = new Gauge({\n  name: 'rabbitmq_queue_messages',\n  help: 'Queue message count',\n  labelNames: ['queue']\n});\n\nsetInterval(async () => {\n  const info = await channel.checkQueue('orders');\n  queueDepth.set({ queue: 'orders' }, info.messageCount);\n}, 10000);", "example": "Track queue depth, consumer lag"},
    {"id": "mq27", "type": "pattern", "name": "Competing Consumers", "description": "Scale consumers horizontally", "pattern": "// Multiple workers process same queue\nfor (let i = 0; i < numWorkers; i++) {\n  const worker = fork('./worker.js');\n  workers.push(worker);\n}\n\n// In worker.js\nchannel.prefetch(10); // Each worker handles 10 at a time\n\nchannel.consume('orders', async (msg) => {\n  await processOrder(JSON.parse(msg.content));\n  channel.ack(msg);\n}, { noAck: false });", "example": "Parallel processing with prefetch"},
    {"id": "mq28", "type": "pattern", "name": "Event Versioning", "description": "Handle schema evolution", "pattern": "// Event with version\nconst event = {\n  type: 'OrderCreated',\n  version: 2,\n  data: { orderId, items, customerId }\n};\n\n// Consumer handles multiple versions\nfunction handleOrderCreated(event) {\n  switch (event.version) {\n    case 1:\n      // Legacy format\n      return handleV1(event.data);\n    case 2:\n      // Current format\n      return handleV2(event.data);\n    default:\n      throw new Error(`Unknown version: ${event.version}`);\n  }\n}", "example": "Backward compatible events"},
    {"id": "mq29", "type": "pattern", "name": "Correlation Context", "description": "Track request flow across services", "pattern": "// Inject correlation ID\nfunction publishWithContext(queue, message, context) {\n  channel.sendToQueue(queue, Buffer.from(JSON.stringify(message)), {\n    headers: {\n      'x-correlation-id': context.correlationId,\n      'x-causation-id': context.messageId,\n      'x-trace-id': context.traceId\n    },\n    messageId: crypto.randomUUID()\n  });\n}\n\n// Extract and propagate\nchannel.consume('orders', (msg) => {\n  const context = {\n    correlationId: msg.properties.headers['x-correlation-id'],\n    causationId: msg.properties.messageId,\n    traceId: msg.properties.headers['x-trace-id']\n  };\n  \n  processWithContext(msg, context);\n});", "example": "End-to-end tracing"},
    {"id": "mq30", "type": "testing", "name": "Queue Testing", "description": "Test message handling", "pattern": "describe('Order Consumer', () => {\n  let channel;\n  \n  beforeEach(async () => {\n    channel = await createTestChannel();\n    await channel.purgeQueue('orders');\n  });\n  \n  it('processes valid orders', async () => {\n    const order = { id: '123', items: [{ sku: 'ABC', qty: 1 }] };\n    \n    // Publish test message\n    channel.sendToQueue('orders', Buffer.from(JSON.stringify(order)));\n    \n    // Wait for processing\n    await waitForProcessing();\n    \n    // Verify side effects\n    const savedOrder = await db.orders.findById('123');\n    expect(savedOrder).toBeDefined();\n    expect(savedOrder.status).toBe('processed');\n  });\n  \n  it('handles invalid messages', async () => {\n    channel.sendToQueue('orders', Buffer.from('invalid json'));\n    \n    await waitForProcessing();\n    \n    // Should be in DLQ\n    const dlqInfo = await channel.checkQueue('orders.dlq');\n    expect(dlqInfo.messageCount).toBe(1);\n  });\n});", "example": "Integration tests for consumers"}
  ]
}
