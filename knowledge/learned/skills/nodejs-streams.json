{
  "category": "nodejs-streams",
  "version": "1.0.0",
  "generatedBy": "Claude (Opus 4.5)",
  "description": "Node.js streams and data processing",
  "skills": [
    {"id": "str01", "type": "concept", "name": "Stream Types", "description": "Four stream types", "pattern": "Readable, Writable, Duplex, Transform", "example": "Data flow patterns"},
    {"id": "str02", "type": "concept", "name": "Why Streams", "description": "Benefits", "pattern": "Memory efficient, process data piece by piece", "example": "Handle large files"},
    {"id": "str03", "type": "readable", "name": "Readable Stream", "description": "Data source", "pattern": "fs.createReadStream('large-file.txt')", "example": "Read in chunks"},
    {"id": "str04", "type": "readable", "name": "Readable Events", "description": "Event handling", "pattern": "stream.on('data', chunk => {}); stream.on('end', () => {})", "example": "Process chunks"},
    {"id": "str05", "type": "readable", "name": "Readable Modes", "description": "Flow modes", "pattern": "Flowing mode (data event) vs Paused mode (read())", "example": "Control flow"},
    {"id": "str06", "type": "writable", "name": "Writable Stream", "description": "Data destination", "pattern": "fs.createWriteStream('output.txt')", "example": "Write in chunks"},
    {"id": "str07", "type": "writable", "name": "Write Method", "description": "Write data", "pattern": "stream.write(chunk); stream.end()", "example": "Send data"},
    {"id": "str08", "type": "writable", "name": "Writable Events", "description": "Event handling", "pattern": "stream.on('finish', () => {}); stream.on('drain', () => {})", "example": "Completion events"},
    {"id": "str09", "type": "pipe", "name": "Pipe Method", "description": "Connect streams", "pattern": "readable.pipe(writable)", "example": "Chain streams"},
    {"id": "str10", "type": "pipe", "name": "Pipeline", "description": "Error-safe pipe", "pattern": "const { pipeline } = require('stream/promises'); await pipeline(src, transform, dest)", "example": "Async pipeline"},
    {"id": "str11", "type": "transform", "name": "Transform Stream", "description": "Modify data", "pattern": "new Transform({ transform(chunk, encoding, callback) { this.push(modified); callback() } })", "example": "Process in-flight"},
    {"id": "str12", "type": "transform", "name": "Zlib Compression", "description": "Gzip streams", "pattern": "const { createGzip, createGunzip } = require('zlib')", "example": "Compress/decompress"},
    {"id": "str13", "type": "transform", "name": "Crypto Stream", "description": "Encryption", "pattern": "crypto.createCipheriv(), crypto.createDecipheriv()", "example": "Stream encryption"},
    {"id": "str14", "type": "duplex", "name": "Duplex Stream", "description": "Read and write", "pattern": "Both readable and writable", "example": "TCP sockets"},
    {"id": "str15", "type": "create", "name": "Custom Readable", "description": "Create readable", "pattern": "class MyReadable extends Readable { _read() { this.push(data) } }", "example": "Custom source"},
    {"id": "str16", "type": "create", "name": "Custom Writable", "description": "Create writable", "pattern": "class MyWritable extends Writable { _write(chunk, encoding, callback) { } }", "example": "Custom destination"},
    {"id": "str17", "type": "backpressure", "name": "Backpressure", "description": "Flow control", "pattern": "write() returns false when buffer full, wait for 'drain'", "example": "Prevent memory overflow"},
    {"id": "str18", "type": "backpressure", "name": "Handle Backpressure", "description": "Pause/resume", "pattern": "if (!writable.write(chunk)) readable.pause(); writable.once('drain', () => readable.resume())", "example": "Manual flow control"},
    {"id": "str19", "type": "error", "name": "Error Handling", "description": "Stream errors", "pattern": "stream.on('error', err => {})", "example": "Each stream needs handler"},
    {"id": "str20", "type": "http", "name": "HTTP Streams", "description": "Request/response", "pattern": "req is Readable, res is Writable", "example": "Stream to response"},
    {"id": "str21", "type": "http", "name": "Stream File Response", "description": "Serve files", "pattern": "fs.createReadStream(file).pipe(res)", "example": "Efficient file serving"},
    {"id": "str22", "type": "async", "name": "Async Iteration", "description": "For await", "pattern": "for await (const chunk of stream) { }", "example": "Modern syntax"},
    {"id": "str23", "type": "async", "name": "Readable.from", "description": "Async to stream", "pattern": "Readable.from(asyncGenerator())", "example": "Async iterable source"},
    {"id": "str24", "type": "buffer", "name": "Collect to Buffer", "description": "Read all", "pattern": "const chunks = []; for await (const chunk of stream) chunks.push(chunk); Buffer.concat(chunks)", "example": "When needed"},
    {"id": "str25", "type": "buffer", "name": "Stream to String", "description": "Text stream", "pattern": "stream.setEncoding('utf8') or chunk.toString()", "example": "Text processing"},
    {"id": "str26", "type": "objectMode", "name": "Object Mode", "description": "Non-buffer streams", "pattern": "new Readable({ objectMode: true })", "example": "Stream objects"},
    {"id": "str27", "type": "highWaterMark", "name": "Buffer Size", "description": "Buffer threshold", "pattern": "new Readable({ highWaterMark: 1024 })", "example": "Control memory"},
    {"id": "str28", "type": "web", "name": "Web Streams API", "description": "Browser-compatible", "pattern": "ReadableStream, WritableStream, TransformStream", "example": "Cross-platform"},
    {"id": "str29", "type": "pattern", "name": "Line-by-Line", "description": "Process lines", "pattern": "readline.createInterface({ input: stream })", "example": "Log processing"},
    {"id": "str30", "type": "pattern", "name": "CSV Processing", "description": "Parse CSV stream", "pattern": "file.pipe(csv.parse()).on('data', row => {})", "example": "Large CSV files"}
  ]
}
