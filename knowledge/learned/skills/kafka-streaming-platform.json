{
  "category": "kafka-streaming-platform",
  "version": "1.0.0",
  "generatedBy": "Claude (Opus 4.5)",
  "description": "Apache Kafka streaming patterns",
  "skills": [
    {"id": "kfk01", "type": "setup", "name": "Install kafkajs", "description": "NPM install", "pattern": "npm install kafkajs", "example": "Install"},
    {"id": "kfk02", "type": "client", "name": "Create Client", "description": "Kafka client", "pattern": "import { Kafka } from 'kafkajs';\n\nconst kafka = new Kafka({\n  clientId: 'my-app',\n  brokers: ['localhost:9092'],\n  ssl: false,\n  sasl: undefined\n});", "example": "Client"},
    {"id": "kfk03", "type": "producer", "name": "Producer", "description": "Send messages", "pattern": "const producer = kafka.producer();\n\nawait producer.connect();\n\nawait producer.send({\n  topic: 'events',\n  messages: [\n    { key: 'user-123', value: JSON.stringify({ action: 'login' }) }\n  ]\n});\n\nawait producer.disconnect();", "example": "Producer"},
    {"id": "kfk04", "type": "batch", "name": "Batch Send", "description": "Multiple messages", "pattern": "await producer.send({\n  topic: 'events',\n  messages: [\n    { key: 'user-1', value: 'event-1' },\n    { key: 'user-2', value: 'event-2' },\n    { key: 'user-3', value: 'event-3' }\n  ]\n});\n\n// Or multiple topics\nawait producer.sendBatch({\n  topicMessages: [\n    { topic: 'topic-1', messages: [{ value: 'msg' }] },\n    { topic: 'topic-2', messages: [{ value: 'msg' }] }\n  ]\n});", "example": "Batch"},
    {"id": "kfk05", "type": "consumer", "name": "Consumer", "description": "Receive messages", "pattern": "const consumer = kafka.consumer({ groupId: 'my-group' });\n\nawait consumer.connect();\nawait consumer.subscribe({ topic: 'events', fromBeginning: true });\n\nawait consumer.run({\n  eachMessage: async ({ topic, partition, message }) => {\n    console.log({\n      key: message.key?.toString(),\n      value: message.value?.toString(),\n      partition,\n      offset: message.offset\n    });\n  }\n});", "example": "Consumer"},
    {"id": "kfk06", "type": "batch", "name": "Batch Consumer", "description": "Process batches", "pattern": "await consumer.run({\n  eachBatch: async ({ batch, resolveOffset, heartbeat }) => {\n    for (const message of batch.messages) {\n      await processMessage(message);\n      resolveOffset(message.offset);\n      await heartbeat();\n    }\n  }\n});", "example": "Batch Consumer"},
    {"id": "kfk07", "type": "partition", "name": "Partitions", "description": "Message ordering", "pattern": "// Messages with same key go to same partition\nawait producer.send({\n  topic: 'orders',\n  messages: [\n    { key: 'user-123', value: 'order-1', partition: 0 },  // Explicit\n    { key: 'user-123', value: 'order-2' }  // Key-based\n  ]\n});", "example": "Partitions"},
    {"id": "kfk08", "type": "offset", "name": "Offset Management", "description": "Control offsets", "pattern": "await consumer.subscribe({ topic: 'events' });\n\n// Seek to offset\nconsumer.seek({ topic: 'events', partition: 0, offset: '100' });\n\n// Seek to timestamp\nawait consumer.run({\n  eachMessage: async ({ topic, partition }) => {\n    consumer.seek({ topic, partition, offset: timestamp.toString() });\n  }\n});", "example": "Offset"},
    {"id": "kfk09", "type": "commit", "name": "Commit Offsets", "description": "Manual commits", "pattern": "await consumer.run({\n  autoCommit: false,\n  eachMessage: async ({ topic, partition, message }) => {\n    await processMessage(message);\n    \n    // Manual commit\n    await consumer.commitOffsets([{\n      topic,\n      partition,\n      offset: (parseInt(message.offset) + 1).toString()\n    }]);\n  }\n});", "example": "Commit"},
    {"id": "kfk10", "type": "admin", "name": "Admin Client", "description": "Manage topics", "pattern": "const admin = kafka.admin();\nawait admin.connect();\n\n// Create topic\nawait admin.createTopics({\n  topics: [{\n    topic: 'new-topic',\n    numPartitions: 3,\n    replicationFactor: 1\n  }]\n});\n\n// List topics\nconst topics = await admin.listTopics();\n\n// Delete topic\nawait admin.deleteTopics({ topics: ['old-topic'] });\n\nawait admin.disconnect();", "example": "Admin"},
    {"id": "kfk11", "type": "groups", "name": "Consumer Groups", "description": "Group management", "pattern": "// List groups\nconst groups = await admin.listGroups();\n\n// Describe group\nconst { groups: [group] } = await admin.describeGroups(['my-group']);\nconsole.log(group.members);\n\n// Reset offsets\nawait admin.resetOffsets({\n  groupId: 'my-group',\n  topic: 'events',\n  earliest: true\n});", "example": "Groups"},
    {"id": "kfk12", "type": "headers", "name": "Message Headers", "description": "Metadata", "pattern": "await producer.send({\n  topic: 'events',\n  messages: [{\n    key: 'user-123',\n    value: JSON.stringify(data),\n    headers: {\n      'correlation-id': 'abc123',\n      'content-type': 'application/json'\n    }\n  }]\n});\n\n// Read headers\neachMessage: async ({ message }) => {\n  const correlationId = message.headers['correlation-id']?.toString();\n}", "example": "Headers"},
    {"id": "kfk13", "type": "compress", "name": "Compression", "description": "Compress messages", "pattern": "import { CompressionTypes } from 'kafkajs';\n\nconst producer = kafka.producer({\n  createPartitioner: Partitioners.DefaultPartitioner\n});\n\nawait producer.send({\n  topic: 'events',\n  compression: CompressionTypes.GZIP,  // or SNAPPY, LZ4, ZSTD\n  messages: [...]\n});", "example": "Compress"},
    {"id": "kfk14", "type": "idempotent", "name": "Idempotent Producer", "description": "Exactly-once", "pattern": "const producer = kafka.producer({\n  idempotent: true,\n  maxInFlightRequests: 5\n});\n\n// Transactional producer\nconst producer = kafka.producer({\n  transactionalId: 'my-transactional-id',\n  maxInFlightRequests: 1\n});\n\nawait producer.transaction();", "example": "Idempotent"},
    {"id": "kfk15", "type": "transaction", "name": "Transactions", "description": "Atomic operations", "pattern": "const transaction = await producer.transaction();\n\ntry {\n  await transaction.send({ topic: 'topic-1', messages: [...] });\n  await transaction.send({ topic: 'topic-2', messages: [...] });\n  await transaction.commit();\n} catch (err) {\n  await transaction.abort();\n  throw err;\n}", "example": "Transaction"},
    {"id": "kfk16", "type": "rebalance", "name": "Rebalance Listener", "description": "Handle rebalance", "pattern": "const consumer = kafka.consumer({\n  groupId: 'my-group',\n  rebalanceTimeout: 30000\n});\n\nconsumer.on(consumer.events.GROUP_JOIN, ({ payload }) => {\n  console.log('Joined group:', payload);\n});\n\nconsumer.on(consumer.events.REBALANCING, () => {\n  console.log('Rebalancing...');\n});", "example": "Rebalance"},
    {"id": "kfk17", "type": "error", "name": "Error Handling", "description": "Handle errors", "pattern": "consumer.on(consumer.events.CRASH, ({ payload: { error } }) => {\n  console.error('Consumer crashed:', error);\n});\n\nproducer.on(producer.events.DELIVERY_REPORT, ({ payload }) => {\n  if (payload.error) {\n    console.error('Delivery failed:', payload.error);\n  }\n});", "example": "Errors"},
    {"id": "kfk18", "type": "retry", "name": "Retry Logic", "description": "Retry config", "pattern": "const kafka = new Kafka({\n  clientId: 'my-app',\n  brokers: ['localhost:9092'],\n  retry: {\n    initialRetryTime: 100,\n    retries: 8,\n    maxRetryTime: 30000,\n    factor: 2\n  }\n});", "example": "Retry"},
    {"id": "kfk19", "type": "ssl", "name": "SSL/TLS", "description": "Secure connection", "pattern": "const kafka = new Kafka({\n  clientId: 'my-app',\n  brokers: ['broker:9093'],\n  ssl: {\n    ca: [fs.readFileSync('ca.pem')],\n    key: fs.readFileSync('client-key.pem'),\n    cert: fs.readFileSync('client-cert.pem')\n  }\n});", "example": "SSL"},
    {"id": "kfk20", "type": "sasl", "name": "SASL Auth", "description": "Authentication", "pattern": "const kafka = new Kafka({\n  clientId: 'my-app',\n  brokers: ['broker:9092'],\n  sasl: {\n    mechanism: 'plain',  // or 'scram-sha-256', 'scram-sha-512'\n    username: 'user',\n    password: 'password'\n  }\n});", "example": "SASL"},
    {"id": "kfk21", "type": "schema", "name": "Schema Registry", "description": "Avro schemas", "pattern": "import { SchemaRegistry } from '@kafkajs/confluent-schema-registry';\n\nconst registry = new SchemaRegistry({ host: 'http://localhost:8081' });\n\n// Encode\nconst encodedValue = await registry.encode(schemaId, data);\n\n// Decode\nconst decodedValue = await registry.decode(message.value);", "example": "Schema"},
    {"id": "kfk22", "type": "streams", "name": "Kafka Streams", "description": "Stream processing", "pattern": "// KafkaJS doesn't have built-in streams\n// Use kafkajs-stream or implement manually\n\nawait consumer.run({\n  eachMessage: async ({ message }) => {\n    const processed = transform(message);\n    await producer.send({\n      topic: 'processed',\n      messages: [processed]\n    });\n  }\n});", "example": "Streams"},
    {"id": "kfk23", "type": "logging", "name": "Logging", "description": "Custom logger", "pattern": "const kafka = new Kafka({\n  clientId: 'my-app',\n  brokers: ['localhost:9092'],\n  logLevel: logLevel.INFO,\n  logCreator: (level) => ({ namespace, log }) => {\n    console.log(`[${namespace}] ${log.message}`);\n  }\n});", "example": "Logging"},
    {"id": "kfk24", "type": "metrics", "name": "Metrics", "description": "Instrumentation", "pattern": "const { Kafka } = require('kafkajs');\nconst { PrometheusExporter } = require('@kafkajs/prometheus-exporter');\n\nconst exporter = new PrometheusExporter();\nconst kafka = new Kafka({ clientId: 'my-app', brokers: [...] });\n\nexporter.observe(kafka);", "example": "Metrics"},
    {"id": "kfk25", "type": "pause", "name": "Pause/Resume", "description": "Flow control", "pattern": "// Pause consuming\nconsumer.pause([{ topic: 'events' }]);\n\n// Resume\nconsumer.resume([{ topic: 'events' }]);\n\n// Pause specific partitions\nconsumer.pause([{ topic: 'events', partitions: [0, 1] }]);", "example": "Pause"},
    {"id": "kfk26", "type": "seek", "name": "Seek", "description": "Jump to offset", "pattern": "await consumer.subscribe({ topic: 'events' });\n\nawait consumer.run({\n  eachMessage: async () => {\n    // Seek to beginning\n    consumer.seek({ topic: 'events', partition: 0, offset: '0' });\n    \n    // Seek to end\n    consumer.seek({ topic: 'events', partition: 0, offset: '-1' });\n  }\n});", "example": "Seek"},
    {"id": "kfk27", "type": "typescript", "name": "TypeScript", "description": "Type safety", "pattern": "import { Kafka, Producer, Consumer, EachMessagePayload } from 'kafkajs';\n\ninterface Event {\n  userId: string;\n  action: string;\n}\n\nconst handleMessage = async ({ message }: EachMessagePayload) => {\n  const event: Event = JSON.parse(message.value?.toString() || '{}');\n};", "example": "TypeScript"},
    {"id": "kfk28", "type": "test", "name": "Testing", "description": "Test consumers", "pattern": "// Use testcontainers\nimport { KafkaContainer } from '@testcontainers/kafka';\n\nconst container = await new KafkaContainer().start();\n\nconst kafka = new Kafka({\n  brokers: [container.getBrokers()[0]]\n});", "example": "Testing"},
    {"id": "kfk29", "type": "docker", "name": "Docker Setup", "description": "Container", "pattern": "# docker-compose.yml\nservices:\n  kafka:\n    image: confluentinc/cp-kafka:latest\n    ports:\n      - '9092:9092'\n    environment:\n      KAFKA_BROKER_ID: 1\n      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181\n      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9092", "example": "Docker"},
    {"id": "kfk30", "type": "pattern", "name": "Best Practices", "description": "Patterns", "pattern": "// 1. Use consumer groups for scalability\n// 2. Set appropriate partitions for throughput\n// 3. Use idempotent producer\n// 4. Handle rebalancing gracefully\n// 5. Monitor lag and consumer health", "example": "Patterns"}
  ]
}
