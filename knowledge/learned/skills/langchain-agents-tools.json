{
  "category": "langchain-agents-tools",
  "version": "1.0.0",
  "generatedBy": "Claude (Opus 4.5)",
  "description": "LangChain agents and tools",
  "skills": [
    {"id": "lc01", "type": "install", "name": "LangChain Install", "description": "Install LangChain", "pattern": "pip install langchain langchain-openai", "example": "Core + provider"},
    {"id": "lc02", "type": "llm", "name": "ChatOpenAI", "description": "OpenAI chat model", "pattern": "from langchain_openai import ChatOpenAI; llm = ChatOpenAI(model='gpt-4')", "example": "Chat completion"},
    {"id": "lc03", "type": "llm", "name": "Ollama LLM", "description": "Local Ollama model", "pattern": "from langchain_community.llms import Ollama; llm = Ollama(model='llama3')", "example": "Local inference"},
    {"id": "lc04", "type": "chain", "name": "Simple Chain", "description": "Prompt + LLM", "pattern": "chain = prompt | llm | parser", "example": "LCEL syntax"},
    {"id": "lc05", "type": "chain", "name": "Runnable Sequence", "description": "Chain components", "pattern": "RunnableSequence.from([step1, step2, step3])", "example": "Sequential processing"},
    {"id": "lc06", "type": "prompt", "name": "Prompt Templates", "description": "Variable prompts", "pattern": "ChatPromptTemplate.from_messages([('system', '...'), ('user', '{input}')])", "example": "Formatted prompts"},
    {"id": "lc07", "type": "prompt", "name": "Few-Shot Prompts", "description": "Example-based", "pattern": "FewShotPromptTemplate with examples", "example": "Learning from examples"},
    {"id": "lc08", "type": "output", "name": "Output Parsers", "description": "Parse LLM output", "pattern": "JsonOutputParser, PydanticOutputParser", "example": "Structured output"},
    {"id": "lc09", "type": "memory", "name": "Conversation Memory", "description": "Chat history", "pattern": "ConversationBufferMemory, ConversationSummaryMemory", "example": "Remember context"},
    {"id": "lc10", "type": "memory", "name": "Vector Store Memory", "description": "Semantic memory", "pattern": "VectorStoreRetrieverMemory", "example": "Long-term memory"},
    {"id": "lc11", "type": "retriever", "name": "Document Retriever", "description": "Search documents", "pattern": "vectorstore.as_retriever(search_kwargs={'k': 4})", "example": "RAG retrieval"},
    {"id": "lc12", "type": "retriever", "name": "RAG Chain", "description": "Retrieval + generation", "pattern": "RetrievalQA.from_chain_type(llm, retriever=retriever)", "example": "Q&A system"},
    {"id": "lc13", "type": "vectorstore", "name": "Chroma VectorStore", "description": "ChromaDB integration", "pattern": "Chroma.from_documents(docs, embeddings)", "example": "Local vector DB"},
    {"id": "lc14", "type": "vectorstore", "name": "FAISS VectorStore", "description": "Facebook AI search", "pattern": "FAISS.from_documents(docs, embeddings)", "example": "Fast similarity"},
    {"id": "lc15", "type": "embeddings", "name": "OpenAI Embeddings", "description": "Text embeddings", "pattern": "OpenAIEmbeddings(model='text-embedding-3-small')", "example": "Vector representation"},
    {"id": "lc16", "type": "embeddings", "name": "Local Embeddings", "description": "HuggingFace embeddings", "pattern": "HuggingFaceEmbeddings(model_name='all-MiniLM-L6-v2')", "example": "Free local"},
    {"id": "lc17", "type": "loader", "name": "Document Loaders", "description": "Load documents", "pattern": "PyPDFLoader, TextLoader, WebBaseLoader", "example": "Various sources"},
    {"id": "lc18", "type": "splitter", "name": "Text Splitters", "description": "Chunk documents", "pattern": "RecursiveCharacterTextSplitter(chunk_size=1000)", "example": "Manageable pieces"},
    {"id": "lc19", "type": "agent", "name": "Create Agent", "description": "LLM with tools", "pattern": "create_react_agent(llm, tools, prompt)", "example": "ReAct pattern"},
    {"id": "lc20", "type": "agent", "name": "Agent Executor", "description": "Run agent", "pattern": "AgentExecutor(agent=agent, tools=tools)", "example": "Execute actions"},
    {"id": "lc21", "type": "tool", "name": "Custom Tools", "description": "Create tools", "pattern": "@tool\\ndef search(query: str) -> str:", "example": "Function as tool"},
    {"id": "lc22", "type": "tool", "name": "Built-in Tools", "description": "Pre-made tools", "pattern": "DuckDuckGoSearchRun, WikipediaQueryRun", "example": "Search, Wikipedia"},
    {"id": "lc23", "type": "tool", "name": "Tool Description", "description": "Describe for LLM", "pattern": "Tool(name='search', description='Search the web')", "example": "LLM decides when"},
    {"id": "lc24", "type": "callback", "name": "Callbacks", "description": "Event handlers", "pattern": "callbacks=[StreamingStdOutCallbackHandler()]", "example": "Logging, streaming"},
    {"id": "lc25", "type": "stream", "name": "Streaming", "description": "Stream responses", "pattern": "for chunk in chain.stream(input): print(chunk)", "example": "Real-time output"},
    {"id": "lc26", "type": "async", "name": "Async Operations", "description": "Async execution", "pattern": "await chain.ainvoke(input)", "example": "Non-blocking"},
    {"id": "lc27", "type": "batch", "name": "Batch Processing", "description": "Process multiple", "pattern": "chain.batch([input1, input2, input3])", "example": "Parallel execution"},
    {"id": "lc28", "type": "hub", "name": "LangChain Hub", "description": "Shared prompts", "pattern": "hub.pull('rlm/rag-prompt')", "example": "Community prompts"},
    {"id": "lc29", "type": "trace", "name": "LangSmith Tracing", "description": "Debug and monitor", "pattern": "LANGCHAIN_TRACING_V2=true", "example": "Observability"},
    {"id": "lc30", "type": "langgraph", "name": "LangGraph", "description": "Stateful agents", "pattern": "StateGraph for complex workflows", "example": "Multi-step agents"}
  ]
}
