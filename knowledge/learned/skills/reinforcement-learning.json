{
  "category": "reinforcement-learning",
  "version": "1.0.0",
  "generatedBy": "Claude (Opus 4.5)",
  "description": "Reinforcement Learning Techniques",
  "skills": [
    {"id": "rl01", "type": "basics", "name": "RL Fundamentals", "description": "Learn from interaction", "pattern": "Agent → Environment → Reward", "example": "Game playing"},
    {"id": "rl02", "type": "basics", "name": "MDP", "description": "Markov Decision Process", "pattern": "States, actions, rewards", "example": "RL framework"},
    {"id": "rl03", "type": "basics", "name": "Policy", "description": "Action selection", "pattern": "π(a|s)", "example": "Decision making"},
    {"id": "rl04", "type": "basics", "name": "Value Function", "description": "Expected return", "pattern": "V(s), Q(s,a)", "example": "State value"},
    {"id": "rl05", "type": "basics", "name": "Bellman Equation", "description": "Recursive value", "pattern": "V = R + γV'", "example": "Value decomposition"},
    {"id": "rl06", "type": "tabular", "name": "Q-Learning", "description": "Value-based", "pattern": "Off-policy TD", "example": "Classic algorithm"},
    {"id": "rl07", "type": "tabular", "name": "SARSA", "description": "On-policy learning", "pattern": "State-action-reward-state-action", "example": "On-policy TD"},
    {"id": "rl08", "type": "tabular", "name": "Monte Carlo Methods", "description": "Episode-based", "pattern": "Full return estimation", "example": "Episodic tasks"},
    {"id": "rl09", "type": "tabular", "name": "TD Learning", "description": "Temporal difference", "pattern": "Bootstrap estimates", "example": "Online learning"},
    {"id": "rl10", "type": "deep", "name": "DQN", "description": "Deep Q-Network", "pattern": "Neural Q-function", "example": "Atari games"},
    {"id": "rl11", "type": "deep", "name": "Double DQN", "description": "Reduce overestimation", "pattern": "Separate networks", "example": "Improved DQN"},
    {"id": "rl12", "type": "deep", "name": "Dueling DQN", "description": "Value and advantage", "pattern": "V + A streams", "example": "Architecture improvement"},
    {"id": "rl13", "type": "deep", "name": "Prioritized Experience Replay", "description": "Important transitions", "pattern": "TD-error priority", "example": "Efficient learning"},
    {"id": "rl14", "type": "policy", "name": "Policy Gradient", "description": "Direct policy optimization", "pattern": "∇J(θ)", "example": "REINFORCE"},
    {"id": "rl15", "type": "policy", "name": "Actor-Critic", "description": "Combined approach", "pattern": "Policy + value network", "example": "A2C, A3C"},
    {"id": "rl16", "type": "policy", "name": "PPO", "description": "Proximal Policy Optimization", "pattern": "Clipped objective", "example": "Stable training"},
    {"id": "rl17", "type": "policy", "name": "TRPO", "description": "Trust Region", "pattern": "KL constraint", "example": "Safe updates"},
    {"id": "rl18", "type": "continuous", "name": "DDPG", "description": "Continuous actions", "pattern": "Deterministic policy", "example": "Robotics control"},
    {"id": "rl19", "type": "continuous", "name": "SAC", "description": "Soft Actor-Critic", "pattern": "Maximum entropy", "example": "Exploration-exploitation"},
    {"id": "rl20", "type": "continuous", "name": "TD3", "description": "Twin Delayed DDPG", "pattern": "Clipped double Q", "example": "Continuous control"},
    {"id": "rl21", "type": "exploration", "name": "Epsilon-Greedy", "description": "Random exploration", "pattern": "ε probability random", "example": "Simple exploration"},
    {"id": "rl22", "type": "exploration", "name": "UCB", "description": "Upper confidence bound", "pattern": "Optimism in uncertainty", "example": "Bandit exploration"},
    {"id": "rl23", "type": "exploration", "name": "Intrinsic Motivation", "description": "Curiosity-driven", "pattern": "Novelty reward", "example": "ICM, RND"},
    {"id": "rl24", "type": "advanced", "name": "Model-Based RL", "description": "Learn environment", "pattern": "World models", "example": "Sample efficient"},
    {"id": "rl25", "type": "advanced", "name": "Hierarchical RL", "description": "Multi-level control", "pattern": "Options framework", "example": "Complex tasks"},
    {"id": "rl26", "type": "advanced", "name": "Multi-Agent RL", "description": "Multiple agents", "pattern": "Cooperative, competitive", "example": "Game theory"},
    {"id": "rl27", "type": "advanced", "name": "Offline RL", "description": "Batch learning", "pattern": "No environment interaction", "example": "Historical data"},
    {"id": "rl28", "type": "tools", "name": "OpenAI Gym", "description": "RL environments", "pattern": "Standard interface", "example": "Benchmarks"},
    {"id": "rl29", "type": "tools", "name": "Stable Baselines3", "description": "RL implementations", "pattern": "PyTorch algorithms", "example": "Production RL"},
    {"id": "rl30", "type": "applications", "name": "RLHF", "description": "Human feedback", "pattern": "Preference learning", "example": "LLM alignment"}
  ]
}
