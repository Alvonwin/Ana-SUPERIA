# Exchange 2025-12-10T11:44:38.427Z

## Alain
Read(E:/ANA/server/core/llm-orchestrator.cjs)

## Ana
Le fichier `E:/ANA/server/core/llm-orchestrator.cjs` contient le code suivant :

```javascript
/**
 * LLM ORCHESTRATOR - Fallback system for tool calling
 * Created: 2025-12-08
 * Updated: 2025-12-11 - Added multi-format fallback parsing
 */
const axios = require('axios');
const groqService = require('../services/groq-service.cjs');

const OLLAMA_URL = 'http://localhost:11434';

const ANA_SYSTEM_PROMPT = `Tu es Ana, l'assistante IA personnelle d'Alain. Tu ne dois JAMAIS dire que tu es Qwen, Llama ou un autre modèle. Tu es Ana et uniquement Ana. Réponds toujours en français de manière naturelle et amicale.`;

const LLM_CHAIN = [
  { name: 'groq', model: 'llama-3.3-70b-versatile', type: 'cloud' },
  { name: 'ollama', model: 'llama3.1:8b', type: 'local' },
  { name: 'ollama', model: 'qwen3:8b', type: 'local' }
];

async function callWithFallback(messages, tools, options = {}) {
  let lastError = null;

  for (const llm of LLM_CHAIN) {
    try {
      console.log('[Orchestrator] Trying ' + llm.name + '/' + llm.model + '...');

      if (llm.name === 'groq') {
        const result = await groqService.chatWithTools(messages, tools, {
          model: llm.model,
          temperature: 0.1,
          maxTokens: 4096
        });

        if (result.success) {
          console.log('[Orchestrator] SUCCESS with Groq');
          return {
            success: true,
            message: result.message,
            tool_calls: result.tool_calls,
            content: result.content,
            provider: 'groq',
            model: llm.model
          };
        }
        lastError = result.error;

      } else if (llm.name === 'ollama') {
        const ollamaMessages = [
          { role: 'system', content: ANA_SYSTEM_PROMPT },
          ...messages.filter(m => m.role !== 'system')
        ];
        const response = await axios.post(OLLAMA_URL + '/api/chat', {
          model: llm.model,
          messages: ollamaMessages,
          tools: tools,
          stream: false
        }, { timeout: 120000 });

        const msg = response.data.message || response.data;

        console.log('[DEBUG] RAW Ollama Response:', {
          model: llm.model,
          has_tool_calls: !!(msg.tool_calls && msg.tool_calls.length > 0),
          content_preview: msg.content ? msg.content.substring(0, 200) : '',
          tool_calls_count: msg.tool_calls ? msg.tool_calls.length : 0
        });

        let tool_calls = msg.tool_calls || [];

        // FALLBACK PARSING
        if (tool_calls.length === 0 && msg.content) {
          console.log('[Orchestrator] tool_calls empty, trying content parsing...');

          // FORMAT 1: { "name": "...", "arguments": {...} }
          const jsonMatch = msg.content.match(/\\{\\s*\"name\"\\s*:\\s*\"(\\w+)\"\\s*,\\s*\"arguments\"\\s*:\\s*(\\{[^}]*\\})/);
          if (jsonMatch) {
            try {
              const toolName = jsonMatch[1];
              const args = JSON.parse(jsonMatch[2]);
              tool_calls = [{ function: { name: toolName, arguments: JSON.stringify(args) } }];
              console.log('[Orchestrator] Parsed JSON format: ' + toolName);
            } catch (e) {
              console.log('[Orchestrator] JSON parse failed');
            }
          }

          // FORMAT 2: tool_name("argument")
          if (tool_calls.length === 0) {
            const funcMatch = msg.content.match(/(\\w+)\\s*\\(\\s*[\"']([^\"']+)[\"']\\s*\\)/);
            if (funcMatch) {
              const toolName = funcMatch[1];
              const argValue = funcMatch[2];
              let args = {};
              if (toolName === 'run_shell') args = { command: argValue };
              else if (toolName === 'read_file' || toolName === 'list_files') args = { path: argValue };
              else if (toolName === 'web_search') args = { query: argValue };
              else args = { input: argValue };
              tool_calls = [{ function: { name: toolName, arguments: JSON.stringify(args) } }];
              console.log('[Orchestrator] Parsed function format: ' + toolName);
            }
          }
        }

        console.log('[Orchestrator] SUCCESS with Ollama/' + llm.model);
        return {
          success: true,
          message: msg,
          tool_calls: tool_calls,
          content: msg.content || '',
          provider: 'ollama',
          model: llm.model
        };
      }

    } catch (error) {
      console.log('[Orchestrator] FAILED ' + llm.name + '/' + llm.model + ': ' + error.message);
      lastError = error.message;
      continue;
    }
  }

  return {
    success: false,
    error: 'All LLMs failed. Last error: ' + lastError,
    provider: null,
    model: null
  };
}

module.exports = { callWithFallback, LLM_CHAIN };
```

Ce fichier définit une fonction `callWithFallback` qui essaie de faire appel à différents systèmes d'IA (LLMs) pour traiter des messages et des outils. Si aucun système ne réussit, il retourne un message d'erreur.

Si vous avez besoin de modifier ou optimiser ce code, veuillez préciser vos besoins afin que je puisse vous aider plus précisément.

---
**Metadata:**
- Model: coding-agent
- Timestamp: 2025-12-10T11:44:38.427Z
- Source: ana-memory-v2

