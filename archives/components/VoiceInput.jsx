import { useState, useRef, forwardRef, useImperativeHandle } from 'react'
import { useTranslation } from 'react-i18next'
import config from './config'

const VoiceInput = forwardRef(({ onTranscript, playSound, handsFreeModeEnabled }, ref) => {
  const { t } = useTranslation()
  const [isRecording, setIsRecording] = useState(false)
  const [status, setStatus] = useState(t('voice.ready'))
  const mediaRecorderRef = useRef(null)
  const audioChunksRef = useRef([])
  const silenceTimeoutRef = useRef(null)
  const audioContextRef = useRef(null)
  const analyserRef = useRef(null)
  const streamRef = useRef(null)
  const startRecordingTimeoutRef = useRef(null)
  const isStartingRef = useRef(false)
  const hasDetectedSoundRef = useRef(false)
  const silenceStartTimeRef = useRef(null)

  const startRecording = async () => {
    // Bloquer si Mode Vocal d√©sactiv√©
    if (!handsFreeModeEnabled) {
      console.log('‚õî Mode Vocal d√©sactiv√© - Capture bloqu√©e')
      setStatus(t('voice.disabled'))
      return
    }

    // D√©bounce: √©viter appels multiples rapides
    if (isStartingRef.current) {
      console.log('‚è∏Ô∏è startRecording d√©j√† en cours, ignor√©')
      return
    }

    // Annuler tout d√©marrage pr√©c√©dent encore en attente
    if (startRecordingTimeoutRef.current) {
      clearTimeout(startRecordingTimeoutRef.current)
    }

    isStartingRef.current = true

    try {
      setStatus(t('voice.starting'))

      // Demander acc√®s micro
      const stream = await navigator.mediaDevices.getUserMedia({
        audio: {
          echoCancellation: true,
          noiseSuppression: true
        }
      })

      streamRef.current = stream

      // Cr√©er AudioContext pour analyse volume
      const audioContext = new (window.AudioContext || window.webkitAudioContext)()
      const analyser = audioContext.createAnalyser()
      const microphone = audioContext.createMediaStreamSource(stream)
      microphone.connect(analyser)
      analyser.fftSize = 512

      audioContextRef.current = audioContext
      analyserRef.current = analyser

      // Cr√©er MediaRecorder
      const mediaRecorder = new MediaRecorder(stream, {
        mimeType: 'audio/webm'
      })

      audioChunksRef.current = []

      mediaRecorder.ondataavailable = (event) => {
        if (event.data.size > 0) {
          audioChunksRef.current.push(event.data)
        }
      }

      mediaRecorder.onstop = async () => {
        setStatus(t('voice.transcribing'))

        // Arr√™ter AudioContext
        if (audioContextRef.current) {
          audioContextRef.current.close()
        }

        // Cr√©er blob audio
        const audioBlob = new Blob(audioChunksRef.current, { type: 'audio/webm' })

        // Envoyer √† Voice Platform pour transcription
        try {
          const formData = new FormData()
          formData.append('audio', audioBlob, 'recording.webm')

          const response = await fetch(`${config.VOICE_BACKEND_URL}/transcribe`, {
            method: 'POST',
            body: formData
          })

          if (!response.ok) {
            throw new Error(`HTTP ${response.status}`)
          }

          const data = await response.json()

          if (data.text) {
            console.log('Transcription brute:', data.text)

            // üõ°Ô∏è FILTRE ANTI-GLITCH
            const text = data.text.trim()

            // Rejeter si trop court (moins de 3 caract√®res)
            if (text.length < 3) {
              console.log('‚ö†Ô∏è Glitch rejet√©: texte trop court')
              setStatus(t('voice.glitchIgnored'))
              return
            }

            // Rejeter les patterns parasites connus
            const glitchPatterns = [
              /^sous-titrage/i,
              /^sous-titre/i,
              /^amara\.org/i,
              /^st['']?\d+/i,
              /^subtitle/i,
              /merci d'avoir regard[√©e]/i,
              /thank you for watching/i,
              /thanks for watching/i
            ]

            if (glitchPatterns.some(pattern => pattern.test(text))) {
              console.log('‚ö†Ô∏è Glitch rejet√©: pattern parasite d√©tect√©')
              setStatus(t('voice.noiseIgnored'))
              return
            }

            // Rejeter si r√©p√©tition excessive (m√™me mot >5 fois)
            const words = text.split(/\s+/)
            const wordCounts = {}
            for (const word of words) {
              wordCounts[word] = (wordCounts[word] || 0) + 1
              if (wordCounts[word] > 5) {
                console.log('‚ö†Ô∏è Glitch rejet√©: r√©p√©tition excessive')
                setStatus(t('voice.repetitionIgnored'))
                return
              }
            }

            // Si tout est OK, envoyer la transcription
            console.log('‚úÖ Transcription valid√©e:', text)
            onTranscript(text)
            setStatus(t('voice.ready'))
          } else {
            setStatus(t('voice.noText'))
          }
        } catch (error) {
          console.warn('‚ö†Ô∏è Voice Platform indisponible (transcription impossible)')
          setStatus(`Voice Platform non lanc√©`)
        }

        // Arr√™ter le stream
        stream.getTracks().forEach(track => track.stop())
      }

      mediaRecorder.start(100) // Collecter donn√©es toutes les 100ms (buffer plus long)
      mediaRecorderRef.current = mediaRecorder
      setIsRecording(true)
      setStatus(t('voice.recording'))

      // Son de d√©but d'enregistrement
      if (playSound) playSound('recording-start')

      // D√©marrer d√©tection de silence
      detectSilence()

    } catch (error) {
      console.error('Erreur micro:', error)
      setStatus(`Erreur: ${error.message}`)
    } finally {
      // Lib√©rer le flag apr√®s un d√©lai de s√©curit√©
      setTimeout(() => {
        isStartingRef.current = false
      }, 500)
    }
  }

  const detectSilence = () => {
    if (!analyserRef.current) return

    // R√©initialiser les refs au d√©but de l'enregistrement
    hasDetectedSoundRef.current = false
    silenceStartTimeRef.current = null
    console.log('üîç D√©tection silence d√©marr√©e')

    const bufferLength = analyserRef.current.frequencyBinCount
    const dataArray = new Uint8Array(bufferLength)
    const SILENCE_THRESHOLD = 15
    const SILENCE_DURATION = 5000 // 5 secondes (plus confortable pour phrases longues)

    const checkAudio = () => {
      if (!analyserRef.current || !mediaRecorderRef.current || mediaRecorderRef.current.state !== 'recording') {
        console.log('‚ö†Ô∏è checkAudio arr√™t√© (analyser ou recorder invalide)')
        return
      }

      analyserRef.current.getByteFrequencyData(dataArray)

      // Calculer volume moyen
      let sum = 0
      for (let i = 0; i < bufferLength; i++) {
        sum += dataArray[i]
      }
      const average = sum / bufferLength

      const now = Date.now()

      if (average > SILENCE_THRESHOLD) {
        // Son d√©tect√©
        console.log(`üîä Son d√©tect√©: ${average.toFixed(1)} > ${SILENCE_THRESHOLD}`)
        hasDetectedSoundRef.current = true
        silenceStartTimeRef.current = null // R√©initialiser le compteur de silence
      } else if (hasDetectedSoundRef.current) {
        // Silence d√©tect√© apr√®s avoir eu du son
        if (!silenceStartTimeRef.current) {
          // Premier moment de silence
          silenceStartTimeRef.current = now
          console.log(`ü§´ D√©but silence (volume: ${average.toFixed(1)})`)
        } else {
          const elapsedSilence = now - silenceStartTimeRef.current
          console.log(`‚è±Ô∏è Silence: ${elapsedSilence}ms / ${SILENCE_DURATION}ms (volume: ${average.toFixed(1)})`)

          if (elapsedSilence >= SILENCE_DURATION) {
            // 2 secondes de silence √©coul√©es
            console.log('‚úÖ 2s de silence d√©tect√©es, arr√™t enregistrement')
            stopRecording()
            return // Ne pas continuer le polling
          }
        }
      } else {
        console.log(`üîá Attente son initial (volume: ${average.toFixed(1)} <= ${SILENCE_THRESHOLD})`)
      }

      // Continuer √† checker toutes les 100ms
      setTimeout(checkAudio, 100)
    }

    checkAudio()
  }

  const stopRecording = () => {
    if (silenceTimeoutRef.current) {
      clearTimeout(silenceTimeoutRef.current)
      silenceTimeoutRef.current = null
    }

    if (mediaRecorderRef.current && mediaRecorderRef.current.state === 'recording') {
      mediaRecorderRef.current.stop()
      setIsRecording(false)

      // Son de fin d'enregistrement
      if (playSound) playSound('recording-stop')
    }

    // Cleanup du stream micro
    if (streamRef.current) {
      streamRef.current.getTracks().forEach(track => {
        track.stop()
        console.log('üõë Track micro arr√™t√©:', track.kind)
      })
      streamRef.current = null
    }

    // Cleanup AudioContext
    if (audioContextRef.current && audioContextRef.current.state !== 'closed') {
      audioContextRef.current.close().then(() => {
        console.log('üîá AudioContext ferm√©')
      })
      audioContextRef.current = null
    }
  }

  // Exposer startRecording au parent via ref
  useImperativeHandle(ref, () => ({
    startRecording
  }))

  return (
    <div style={{ marginBottom: '8px', textAlign: 'center' }}>
      <button
        onClick={isRecording ? stopRecording : startRecording}
        style={{
          padding: '12px 24px',
          fontSize: '12.8px',
          backgroundColor: isRecording ? '#ef4444' : '#3b82f6',
          color: 'white',
          border: 'none',
          borderRadius: '6.4px',
          cursor: 'pointer',
          fontWeight: 'bold',
          transition: 'all 0.2s'
        }}
      >
        {isRecording ? t('voice.stop') : t('voice.start')}
      </button>

      <div style={{ marginTop: '6.4px', fontSize: '10.4px', color: '#9ca3af', minHeight: '16px' }}>
        {status}
      </div>
    </div>
  )
})

VoiceInput.displayName = 'VoiceInput'

export default VoiceInput
