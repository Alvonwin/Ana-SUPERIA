/**
 * LLM ORCHESTRATOR - Fallback system for tool calling
 * Created: 2025-12-08
 * 
 * Order:
 *   1. Groq (cloud) - llama-3.3-70b-versatile
 *   2. llama3.1:8b (local Ollama)
 *   3. qwen3:8b (local Ollama)
 */

const axios = require('axios');
const groqService = require('../services/groq-service.cjs');

const OLLAMA_URL = 'http://localhost:11434';

// LLM fallback chain - ORDER MATTERS
const LLM_CHAIN = [
  { name: 'groq', model: 'llama-3.3-70b-versatile', type: 'cloud' },
  { name: 'ollama', model: 'llama3.1:8b', type: 'local' },
  { name: 'ollama', model: 'qwen3:8b', type: 'local' }
];

/**
 * Call LLM with automatic fallback
 * Tries each provider in order until one succeeds
 */
async function callWithFallback(messages, tools, options = {}) {
  let lastError = null;
  
  for (const llm of LLM_CHAIN) {
    try {
      console.log(`[Orchestrator] Trying ${llm.name}/${llm.model}...`);
      
      if (llm.name === 'groq') {
        // Groq cloud call
        const result = await groqService.chatWithTools(messages, tools, {
          model: llm.model,
          temperature: 0.1,
          maxTokens: 4096
        });
        
        if (result.success) {
          console.log(`[Orchestrator] SUCCESS with Groq`);
          return {
            success: true,
            message: result.message,
            tool_calls: result.tool_calls,
            content: result.content,
            provider: 'groq',
            model: llm.model
          };
        }
        lastError = result.error;
        
      } else if (llm.name === 'ollama') {
        // Ollama local call
        const response = await axios.post(`${OLLAMA_URL}/api/chat`, {
          model: llm.model,
          messages: messages,
          tools: tools,
          stream: false
        }, { timeout: 120000 });
        
        const msg = response.data.message || response.data;
        console.log(`[Orchestrator] SUCCESS with Ollama/${llm.model}`);
        
        return {
          success: true,
          message: msg,
          tool_calls: msg.tool_calls || [],
          content: msg.content || '',
          provider: 'ollama',
          model: llm.model
        };
      }
      
    } catch (error) {
      console.log(`[Orchestrator] FAILED ${llm.name}/${llm.model}: ${error.message}`);
      lastError = error.message;
      continue; // Try next in chain
    }
  }
  
  // All failed
  return {
    success: false,
    error: `All LLMs failed. Last error: ${lastError}`,
    provider: null,
    model: null
  };
}

module.exports = {
  callWithFallback,
  LLM_CHAIN
};
